# Gunicorn Workers & Architecture FAQ

Common questions about how Gunicorn workers, ports, and the master-worker architecture work in this Flask + LaunchDarkly application.

## Table of Contents

1. [Do workers automatically get created?](#do-workers-automatically-get-created)
2. [Which port do workers listen on?](#which-port-do-workers-listen-on)
3. [How does the master-worker architecture work?](#how-does-the-master-worker-architecture-work)
4. [What's the difference between workers and threads?](#whats-the-difference-between-workers-and-threads)
5. [How many concurrent requests can be handled?](#how-many-concurrent-requests-can-be-handled)
6. [How does load balancing work?](#how-does-load-balancing-work)

---

## Do workers automatically get created?

**Yes!** Workers are automatically created based on your `gunicorn.conf.py` configuration.

### Configuration:
```python
# gunicorn.conf.py - Line 6
workers = 2  # Gunicorn will automatically create 2 worker processes
threads = 2  # Each worker will have 2 threads
```

### What happens when you start Gunicorn:

```bash
# You run:
gunicorn --config gunicorn.conf.py app:app

# Or with Docker:
docker-compose up
```

### Automatic startup sequence:

```
1. Master Process Starts
   â””â”€ Reads workers = 2 from config
   â””â”€ Loads app (because preload_app = True)
   â””â”€ Initializes LaunchDarkly client

2. Master Automatically Forks Workers
   â”œâ”€ Worker 1 created with PID (e.g., 18)
   â”‚  â””â”€ postfork() called automatically
   â”‚  â””â”€ 2 threads created
   â”‚
   â””â”€ Worker 2 created with PID (e.g., 30)
      â””â”€ postfork() called automatically
      â””â”€ 2 threads created

3. Workers Start Listening
   â””â”€ Ready to handle requests
```

### Real example from logs:

```
[INFO] Starting gunicorn 23.0.0
[INFO] Using worker: gthread
[INFO] Booting worker with pid: 18              â† Worker 1 auto-created
[INFO] Booting worker with pid: 30              â† Worker 2 auto-created
[INFO] âœ“ LaunchDarkly postfork() completed successfully in worker 18
[INFO] âœ“ LaunchDarkly postfork() completed successfully in worker 30
```

**Everything is 100% automatic!** No manual worker management needed.

---

## Which port do workers listen on?

**All workers listen on the SAME port** - they don't each get their own port.

### Configuration:
```python
# gunicorn.conf.py - Line 5
bind = "0.0.0.0:8000"  # All workers share this single port
```

### Master-Worker Port Architecture:

```
                    Port 8000
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Master Process â”‚ â† ONLY the master binds to port 8000
              â”‚   (PID: 1)      â”‚ â† Accepts incoming connections
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”‚ Distributes requests to workers
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚
          â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Worker 1 â”‚             â”‚ Worker 2 â”‚
    â”‚ (PID: 18)â”‚             â”‚ (PID: 30)â”‚
    â”‚ No Port  â”‚             â”‚ No Port  â”‚ â† Workers DON'T bind to ports
    â”‚ 2 threadsâ”‚             â”‚ 2 threadsâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Points:

1. âœ… **Master binds to port**: Only master process binds to `0.0.0.0:8000`
2. âœ… **Workers don't have ports**: Workers communicate with master via internal IPC
3. âœ… **Single entry point**: All requests come through port 8000
4. âœ… **Internal distribution**: Master distributes work to available workers

### Why don't workers need their own ports?

Workers communicate with the master through **Unix sockets** or **pipes** (inter-process communication), not TCP ports. This is:
- âœ… More efficient
- âœ… Lower overhead
- âœ… Automatic load balancing
- âœ… Easier to manage

---

## How does the master-worker architecture work?

Gunicorn uses a **pre-fork worker model** where the master process manages multiple worker processes.

### Architecture Diagram:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Docker Container                    â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            Master Process (PID: 1)             â”‚ â”‚
â”‚  â”‚                                                â”‚ â”‚
â”‚  â”‚  â€¢ Binds to 0.0.0.0:8000                      â”‚ â”‚
â”‚  â”‚  â€¢ Accepts incoming connections                â”‚ â”‚
â”‚  â”‚  â€¢ Distributes to workers                      â”‚ â”‚
â”‚  â”‚  â€¢ Monitors worker health                      â”‚ â”‚
â”‚  â”‚  â€¢ Restarts crashed workers                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                        â”‚                             â”‚
â”‚                        â”‚ fork()                      â”‚
â”‚                        â”‚                             â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚        â”‚                               â”‚            â”‚
â”‚        â–¼                               â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Worker 1 â”‚                   â”‚ Worker 2 â”‚       â”‚
â”‚  â”‚ PID: 18  â”‚                   â”‚ PID: 30  â”‚       â”‚
â”‚  â”‚          â”‚                   â”‚          â”‚       â”‚
â”‚  â”‚ Thread 1 â”‚                   â”‚ Thread 1 â”‚       â”‚
â”‚  â”‚ Thread 2 â”‚                   â”‚ Thread 2 â”‚       â”‚
â”‚  â”‚          â”‚                   â”‚          â”‚       â”‚
â”‚  â”‚ LD Clientâ”‚                   â”‚ LD Clientâ”‚       â”‚
â”‚  â”‚ postfork â”‚                   â”‚ postfork â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†‘
                    â”‚
            HTTP Requests on Port 8000
```

### Request Flow:

```
1. Client sends request
   â†“
2. Request arrives at port 8000
   â†“
3. Master accepts connection
   â†“
4. Master selects available worker
   â†“
5. Worker processes request
   â†“
6. Worker evaluates LaunchDarkly flags
   â†“
7. Worker sends response back
   â†“
8. Master forwards response to client
```

### Master's Responsibilities:

1. âœ… Bind to the configured port
2. âœ… Fork worker processes
3. âœ… Distribute requests to workers
4. âœ… Monitor worker health
5. âœ… Restart crashed workers
6. âœ… Graceful shutdowns
7. âœ… Handle signals (reload, shutdown)

### Worker's Responsibilities:

1. âœ… Process HTTP requests
2. âœ… Run application code
3. âœ… Evaluate LaunchDarkly flags
4. âœ… Handle multiple threads
5. âœ… Send responses

---

## What's the difference between workers and threads?

Both allow concurrent processing, but they work differently.

### Workers (Processes)

```python
workers = 2  # 2 separate processes
```

**Characteristics:**
- âœ… Separate memory space (isolated)
- âœ… True parallelism (multi-core)
- âœ… More memory (each has its own copy)
- âœ… Crash isolation (one worker crashes, others continue)
- âš ï¸ Higher overhead (forking is expensive)

### Threads (Within a worker)

```python
threads = 2  # 2 threads per worker
```

**Characteristics:**
- âœ… Shared memory space
- âœ… Lower memory overhead
- âœ… Fast creation/switching
- âš ï¸ Python GIL limits (good for I/O, not CPU)
- âš ï¸ Less isolation (error can affect thread pool)

### In Our Configuration:

```python
workers = 2   # 2 processes
threads = 2   # 2 threads per process
```

**Total capacity**: 2 workers Ã— 2 threads = **4 concurrent connections**

### Visual Comparison:

```
WORKERS (Processes)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker 1   â”‚    â”‚  Worker 2   â”‚
â”‚  (Process)  â”‚    â”‚  (Process)  â”‚
â”‚             â”‚    â”‚             â”‚
â”‚  Memory: 50MBâ”‚   â”‚  Memory: 50MBâ”‚
â”‚  CPU: Core 1â”‚    â”‚  CPU: Core 2â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘ Isolated         â†‘ Isolated
     
THREADS (Within Worker 1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Worker 1 Process      â”‚
â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Thread 1 â”‚ â”‚ Thread 2 â”‚ â”‚
â”‚  â”‚          â”‚ â”‚          â”‚ â”‚
â”‚  â”‚ Shared   â”‚ â”‚ Shared   â”‚ â”‚
â”‚  â”‚ Memory   â”‚ â”‚ Memory   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### When to use what?

**More Workers (processes)**:
- âœ… CPU-intensive tasks
- âœ… Need process isolation
- âœ… Multi-core utilization
- âœ… Crash resilience

**More Threads**:
- âœ… I/O-bound operations (like LaunchDarkly API calls)
- âœ… Lower memory footprint
- âœ… Shared state needed
- âœ… Quick context switching

**Our setup (2 workers Ã— 2 threads)**: Balanced approach good for most web apps with I/O operations.

---

## How many concurrent requests can be handled?

**Answer: 4 concurrent requests** with the default configuration.

### Calculation:

```python
# gunicorn.conf.py
workers = 2   # Number of processes
threads = 2   # Threads per process

# Total concurrent capacity:
2 workers Ã— 2 threads = 4 concurrent requests
```

### Request Handling:

```
Request Flow on Port 8000:

Request 1 â†’ Master â†’ Worker 1 (Thread 1) âœ“ Handling
Request 2 â†’ Master â†’ Worker 1 (Thread 2) âœ“ Handling
Request 3 â†’ Master â†’ Worker 2 (Thread 1) âœ“ Handling
Request 4 â†’ Master â†’ Worker 2 (Thread 2) âœ“ Handling
Request 5 â†’ Master â†’ Queue (waiting for available thread)
Request 6 â†’ Master â†’ Queue (waiting for available thread)
```

### Scaling Up:

To handle more concurrent requests, increase workers or threads:

```python
# Option 1: More workers (better for CPU tasks)
workers = 4   # 4 workers Ã— 2 threads = 8 concurrent
threads = 2

# Option 2: More threads (better for I/O tasks)
workers = 2   
threads = 4   # 2 workers Ã— 4 threads = 8 concurrent

# Option 3: Both (maximum capacity)
workers = 4   
threads = 4   # 4 workers Ã— 4 threads = 16 concurrent
```

### Rule of Thumb:

```python
# For CPU-bound applications:
workers = (2 Ã— num_cores) + 1

# For I/O-bound applications (like ours with LaunchDarkly):
workers = 2-4
threads = 2-4
```

### LaunchDarkly Consideration:

**Important Note**: The LaunchDarkly SDK **creates its own internal threads automatically** for:
- Background streaming (flag updates)
- Event processing (analytics)

These are **separate from Gunicorn worker threads**. You can set `threads` to any value â‰¥ 1 based on your HTTP concurrency needs. What matters for LaunchDarkly is calling `postfork()` after forking so the SDK can recreate its internal threads.

```python
threads = 2  # Good for I/O concurrency, not a LaunchDarkly requirement
```

Reference: [LaunchDarkly Python SDK Documentation](https://docs.launchdarkly.com/sdk/server-side/python/#considerations-with-worker-based-servers)

---

## How does load balancing work?

Gunicorn's master process automatically distributes requests across workers.

### Load Balancing Strategy:

Gunicorn uses a **simple round-robin** or **least-busy** strategy depending on the worker class.

```
Request Distribution:

Time    Request    Master Decision       Worker Used
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T1      Req 1   â†’  Worker 1 idle      â†’  Worker 1
T2      Req 2   â†’  Worker 2 idle      â†’  Worker 2
T3      Req 3   â†’  Worker 1 available â†’  Worker 1
T4      Req 4   â†’  Worker 2 available â†’  Worker 2
T5      Req 5   â†’  All busy, queue    â†’  (waiting)
```

### Factors Master Considers:

1. âœ… **Worker availability**: Is worker ready to accept?
2. âœ… **Thread availability**: Does worker have free threads?
3. âœ… **Worker health**: Is worker responsive?
4. âœ… **Fairness**: Distribute evenly

### Thread-Level Load Balancing:

Within each worker, threads handle requests concurrently:

```
Worker 1 Load Distribution:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Worker 1             â”‚
â”‚                            â”‚
â”‚  Thread 1: [Request A]     â”‚ â† Processing
â”‚  Thread 2: [Request B]     â”‚ â† Processing
â”‚                            â”‚
â”‚  Queue: [C, D, E]          â”‚ â† Waiting
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Automatic Features:

1. âœ… **Automatic restart**: If worker crashes, master spawns new one
2. âœ… **Graceful shutdown**: Workers finish current requests before stopping
3. âœ… **Health checks**: Master monitors worker responsiveness
4. âœ… **Signal handling**: Reload workers without downtime

### Monitoring Load:

Check active connections and worker status:
```bash
# View Gunicorn stats
docker-compose logs -f

# Look for:
[INFO] Booting worker with pid: 18
[INFO] Worker timeout (pid:30)    â† Worker issue
[INFO] Shutting down: Master
```

### What if a worker crashes?

```
1. Worker crashes or hangs
   â†“
2. Master detects unresponsive worker
   â†“
3. Master kills unresponsive worker
   â†“
4. Master spawns new worker
   â†“
5. New worker calls postfork()
   â†“
6. Service continues with minimal disruption
```

**Result**: High availability with automatic recovery! ğŸ¯

---

## Summary

### Key Takeaways:

1. ğŸ”§ **Workers are automatic** - Created based on config, no manual management
2. ğŸ”Œ **Single port for all** - Master binds to port, distributes to workers
3. âš–ï¸ **Load balancing built-in** - Master automatically distributes requests
4. ğŸ§µ **Workers â‰  Threads** - Processes vs threads, different use cases
5. ğŸ“Š **Capacity = workers Ã— threads** - 2Ã—2 = 4 concurrent in our setup
6. ğŸ”„ **Auto-recovery** - Master restarts crashed workers automatically

### Configuration Reference:

```python
# gunicorn.conf.py
bind = "0.0.0.0:8000"      # Port master listens on
workers = 2                 # Number of worker processes
threads = 2                 # Threads per worker
preload_app = True         # Load before forking
timeout = 30               # Worker timeout (seconds)
```

### Your Setup Performance:

- **Port**: 8000 (single entry point)
- **Workers**: 2 processes
- **Threads**: 4 total (2 per worker)
- **Concurrent requests**: 4
- **LaunchDarkly**: postfork() working correctly âœ“

Perfect for a production web application! ğŸš€

---

## Related Documentation

- [LAUNCHDARKLY_ARCHITECTURE.md](LAUNCHDARKLY_ARCHITECTURE.md) - Deep dive into LaunchDarkly integration
- [DOCKER_QUICKSTART.md](DOCKER_QUICKSTART.md) - Quick start guide
- [README.md](README.md) - Project overview
- [Gunicorn Documentation](https://docs.gunicorn.org/) - Official Gunicorn docs 